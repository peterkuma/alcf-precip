#!/usr/bin/env python3
import warnings
warnings.filterwarnings('ignore')

import sys
import re
import os
import random
import numpy as np
import ds_format as ds
import aquarius_time as aq
import matplotlib.pyplot as plt
import pst
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Model
from keras.layers.merge import concatenate
from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, \
	AveragePooling2D, MaxPooling2D, Flatten, Dense, Dropout, \
	BatchNormalization, Activation
from tensorflow.keras.utils import Sequence

def get_loss_func(nclasses):
	def loss_func(y_true, y_pred):
		a = y_true[:,:,:,0:nclasses]
		b = y_true[:,:,:,nclasses:(2*nclasses)]
		c = y_pred[:,:,:,0:nclasses]
		#c = tf.nn.sigmoid(y_pred[:,:,:,0:nclasses])
		#c = tf.minimum(tf.maximum(c, 0), 1)
		d = a*tf.experimental.numpy.log(c) + \
			(b - a)*tf.experimental.numpy.log(1 - c)
		#d = (b - a)*tf.experimental.numpy.log(1 - c)
		x = -tf.experimental.numpy.sum(d)
		#x = tf.experimental.numpy.sum((c - a)**2)
		return x
	return loss_func

def unet_encoder(x, n, kernel_size=3, batchnorm=True, dropout=0, maxpool=True):
	y = Conv2D(n, (kernel_size, kernel_size), kernel_initializer='he_normal', \
		padding='same')(x)
	if batchnorm:
		y = BatchNormalization()(y)
	y = Activation('relu')(y)
	y = Conv2D(n, (kernel_size, kernel_size), kernel_initializer='he_normal', \
		padding='same')(x)
	if batchnorm:
		y = BatchNormalization()(y)
	y = Activation('relu')(y)
	z = y
	if maxpool:
		y = AveragePooling2D((2, 2))(y)
	if dropout > 0:
		y = Dropout(dropout)(y)
	return y, z

def unet_decoder(x, z, n, kernel_size=3, batchnorm=True, dropout=0):
	y = Conv2DTranspose(n, (kernel_size, kernel_size), strides=(2, 2),
		padding='same')(x)
	y = concatenate([y, z])
	y = Dropout(dropout)(y)
	y, _ = unet_encoder(y, n, kernel_size, batchnorm, 0, False)
	return y

def unet_model(x, nclasses, n=16, dropout=0.1, batchnorm=True):
	y, c1 = unet_encoder(x, n*1, 3, batchnorm, dropout)
	y, c2 = unet_encoder(y, n*2, 3, batchnorm, dropout)
	#y, c3 = unet_encoder(y, n*4, 3, batchnorm, dropout)
	#y, c4 = unet_encoder(y, n*8, 3, batchnorm, dropout)
	y, c5 = unet_encoder(y, n*4, 3, batchnorm, 0, False)
	#y = unet_decoder(y, c4, n*8, 3, batchnorm, dropout)
	#y = unet_decoder(y, c3, n*4, 3, batchnorm, dropout)
	y = unet_decoder(y, c2, n*2, 3, batchnorm, dropout)
	y = unet_decoder(y, c1, n*1, 3, batchnorm, dropout)
	y = Conv2D(nclasses*2, (1, 1), activation='sigmoid')(y)
	return Model(inputs=[x], outputs=[y])

if __name__ == '__main__':
	tf.config.threading.set_inter_op_parallelism_threads(24)

	train_images = np.ones((920, 12, 300, 1), np.float64)
	train_labels = np.ones((920, 12, 300, 1), np.float64)
	train_labels[:,:,:,:] = 0.1

	policy = tf.keras.mixed_precision.Policy('float64')
	tf.keras.mixed_precision.set_global_policy(policy)

	input_ = Input((12, 300, 1))
	model = unet_model(input_, 4, 16, 0.1, True)

	model.compile(optimizer='adam',
				  #loss=get_loss_func(4),
				  loss=tf.keras.losses.MeanSquaredError(),
				  metrics=['accuracy'])

	history = model.fit(train_images, train_labels,
		epochs=10,
	)

	stats1 = model.predict(train_images)
	#stats1 = tf.nn.sigmoid(stats1)
	#print(np.mean(stats1.numpy()))
	print(np.mean(stats1))
